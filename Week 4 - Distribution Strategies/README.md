## Week 4 - Distributed Training
This week, you will harness the power of distributed training to process more data and train larger models, faster. You’ll get an overview of various distributed training strategies and then practice working with two strategies, one that trains on multiple GPU cores, and the other that trains on multiple TPU cores. Get your cape ready, because you’re going to get some superpowers this week!
### Learning Objectives
- Explain how distributed training is different from regular model training
- Use the Mirrored Strategy to train a model on multiple GPUs on the same device
- Use the TPU Strategy to train on multiple cores of a TPU














































